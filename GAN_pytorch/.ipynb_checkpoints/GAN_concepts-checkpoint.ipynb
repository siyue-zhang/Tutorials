{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d130ab54",
   "metadata": {},
   "source": [
    "# Build Basic Generative Adversarial Networks (GANs)\n",
    "Deeplearning.ai study notes\n",
    "\n",
    "https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b0863",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "Differentiable non-linear function\n",
    "- differentiable for backpropagation\n",
    "- non-linear to compute complex features\n",
    "\n",
    "**ReLU**: $max(z,0)$, derivative at zero is set to zero, derivative in negative space is zero -> dying ReLU problem\n",
    "\n",
    "**Leaky ReLU**: slope a = 0.1, catching up popularity\n",
    "\n",
    "**Sigmoid**: output [0,1], sigmoid not often used in hidden layers because the tails of the sigmoid curve have zero derivative -> vanishing gradient problem\n",
    "\n",
    "**Tanh**: output [-1,1], keeps the sign of input z, same saturation problem as sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6bdcb9",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "Training data uses batch stats\n",
    "\n",
    "Test data uses training stats\n",
    "\n",
    "**Covariate shift**: changes in the distribution of one variable affect the distribution of relayed variables\n",
    "\n",
    "Batch normalization:\n",
    "1. reduce internal covariate shift\n",
    "2. smooth out cost function\n",
    "3. make neural network easier to train\n",
    "\n",
    "### Training\n",
    "\n",
    "In a batch, z -> normalized z by mean and std -> learnable scale factor $\\gamma$ + shift factor $\\beta$ -> y -> into activation function\n",
    "\n",
    "$\\gamma$ and $\\beta$ are learnable parameters to get the optimal dist.\n",
    "\n",
    "Not necessary zero mean and standard deviation of one\n",
    "\n",
    "### Testing\n",
    "\n",
    "Use running mean and standard deviation that was computed over the entire training set, fixed after training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b05646",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "Each filter tels different features of the image (eye filter, noise filter, ear filter...)\n",
    "\n",
    "Stride\n",
    "\n",
    "Padding: frame on the image, capture useful information at edge\n",
    "\n",
    "Pooling: lower the dimension of input images\n",
    "\n",
    "Upsampling: (use predifined methods)\n",
    "- Nearest Neighbors\n",
    "- Linear interpolation\n",
    "- Bi-linear interpolation\n",
    "\n",
    "**Transposed Convolutions**\n",
    "\n",
    "input * filter -> up-sampling with learned parameters\n",
    "\n",
    "issue: centered values are influenced much more heavily -> output has checkerboard problem\n",
    "\n",
    "solution: use up-sampling followed by conv -> popular technique t avoid checkerboard problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd728a",
   "metadata": {},
   "source": [
    "## Generative Models\n",
    "\n",
    "* Variational Autoencoders (VAE)\n",
    "\n",
    "Training: img -> Encoder -> Latent Space -> Decoder -> img' (reconstruct image)\n",
    "\n",
    "Generating: random latent space -> Decoder -> generated img\n",
    "\n",
    "* Generative Adversarial Networks (GAN)\n",
    "\n",
    "Training:\n",
    "\n",
    "random noise -> **Generator** -> generated img\n",
    "\n",
    "generated img -> **Discriminator** -> real/fake img\n",
    "\n",
    "Generating:\n",
    "\n",
    "random noise -> Generator -> generated img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca5550",
   "metadata": {},
   "source": [
    "## Procedures\n",
    "\n",
    "Discriminator Training:\n",
    "\n",
    "<img src=\"./images/gan_train.png\" width=\"600\"/>\n",
    "\n",
    "Generator Training: (G wants to fool D as much as possible)\n",
    "\n",
    "<img src=\"./images/gan_train2.png\" width=\"600\"/>\n",
    "\n",
    "Both models should improve together and always be at a similar \"skill\" level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef05f4f",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy (BCE) Loss\n",
    "\n",
    "Deisgned for classification real/fake\n",
    "\n",
    "<img src=\"./images/BCE.png\" width=\"600\"/>\n",
    "\n",
    "- m: batch size\n",
    "- h: prediction made by model\n",
    "- y: true label\n",
    "- x: features (img)\n",
    "- $\\theta$: discriminator parameters\n",
    "\n",
    "when prediction is 0 -> left part = 0<br>\n",
    "when prediction is 1 -> left part tells if h = 0, loss = inf; if h = 1, loss = 0\n",
    "\n",
    "when prediction is 1 -> right part = 0<br>\n",
    "when prediction is 0 -> right part tells if h = 0, loss = 0; if h = 1, loss = inf\n",
    "\n",
    "J -> 0 when label and prediction are close<br>\n",
    "J -> inf when label and prediction are different\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e704dcf",
   "metadata": {},
   "source": [
    "### Problem with BCE\n",
    "\n",
    "When discriminator improves too much, the function approximated by BCE loss will contain flat regions.\n",
    "\n",
    "Flat region on the cost function = vanishing gradients\n",
    "\n",
    "The discriminator does not output useful gradients (feedback) for the generator when the real/fake distributions are far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c8aa7",
   "metadata": {},
   "source": [
    "### Model Collapse\n",
    "\n",
    "Mode: any peak on the density function is a model\n",
    "\n",
    "<img src=\"./images/multi-mode.png\" width=\"600\"/>\n",
    "\n",
    "Real-world datasets have many modes related to each possible class\n",
    "\n",
    "Mode collapse happens when generataor gets stuck in one mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af40ff1",
   "metadata": {},
   "source": [
    "## Earth Mover's Distance (EMD)\n",
    "\n",
    "It measures the distance between two distributions.\n",
    "\n",
    "Amount of efforts to make the generated distribution equal to the real distribution.\n",
    "\n",
    "Example: amount of dirt moved * moving distance to move and transform a pile of dirct in the shape of distribution P to the shape of distribution Q. \n",
    "\n",
    "A recursion problem: $cost_{i+1} = cost_{i} + P_i - Q_i$.\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/EM_distance_discrete.png\n",
    "\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n",
    "No saturation when two distributions are very far apart\n",
    "\n",
    "## Wasserstein GAN\n",
    "\n",
    "W-Loss = $\\mathbb{E}(c(x)-\\mathbb{E}(c(g(z))))$<br>\n",
    "W-Loss helps with mode collapse and vanishing gradient problems\n",
    "\n",
    "### Condition on W-Loss\n",
    "\n",
    "Critic should be 1-Lipschitz continuous, norm of its gradient needs t be at most one.<br>\n",
    "Needed for training stable neural networks with W-Loss.\n",
    "\n",
    "### Weight Clipping\n",
    "Force the weights of critic to a fixed interval<br>\n",
    "Limits the learning ability of critic\n",
    "\n",
    "### Gradient Penalty\n",
    "Softer way, add regularization term - a two sided-penalty on gradients. In practice, a random interpolation image generated from real and fake images are used to calculate the gradient. \n",
    "\n",
    "<img src=\"./images/WGAN.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The authors recommended **RMSProp optimizer** on the critic, rather than a momentum based optimizer such as Adam which could cause instability in the model training. No theoretical explanation yet.\n",
    "\n",
    "### Spectral Normalization\n",
    "\n",
    "Spectral Normalization for Generative Adversarial Networks (Miyato et al. 2018)<br>\n",
    "https://arxiv.org/abs/1802.05957\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb2dc7",
   "metadata": {},
   "source": [
    "## Noise vector $z$\n",
    "\n",
    "The noise vector $z$ has the important role of making sure the images generated from the same class $y$ don't all look the sameâ€”think of it as a random seed. You generate it randomly, usually by sampling random numbers either between 0 and 1 uniformly, or from the normal distribution, which you can denote $z$ ~ $N(0, 1)$. The zero means the normal distribution has a mean of zero, and the 1 means that the normal distribution has a variance of 1. \n",
    "\n",
    "In reality, $z$ is usually larger than just 1 value to allow for more combinations of what $z$ could be. There's no special number that determines what works, but 100 is standard. Some researchers might use a power of 2, like 128 or 512, but again, nothing special about the number itself, just that it's large enough to contain a lot of possibilities. As a result, you would sample $z$ from that many different dimensions (constituting multiple normal distributions).\n",
    "\n",
    "*Fun Fact: this is also called a spherical normal and denoted $z$ ~ $N(0, I)$ where the $I$ represents the identity matrix and means the variance is 1 in all dimensions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b68e54",
   "metadata": {},
   "source": [
    "## Truncation trick\n",
    "\n",
    "So now that you're a bit familiar with noise vectors, here's another cool concept that people use to tune their outputs. It's called the truncation trick. I like to think of the truncation trick as a way of trading off fidelity (quality) and diversity in the samples. It works like this: when you randomly sample your noise vector $z$, you can choose to keep that random $z$ or you can sample another one. \n",
    "\n",
    "Why would you want to sample another one? \n",
    "\n",
    "Well, since I'm sampling $z$ from a normal distribution, my model will see more of those $z$ values within a standard deviation from the mean than those at the tails of the distributionâ€”and this happens during training. This means that while the model is training, it's likely to be familiar with certain noise vectors and as a result model those areas coming from familiar noise vector regions. In these areas, my model will likely have much more realistic results, but nothing too funky, it's not taking as many risks in those regions mapped from those familiar noise vectors. This is the trade-off between fidelity (realistic, high quality images) and diversity (variety in images). \n",
    "\n",
    "What the truncation trick does is resamples the noise vector $z$ until it falls within some bounds of the normal distribution. In fact, it samples $z$ from a truncated normal distribution where the tails are cut off at different values (red line in graph is truncated normal, blue is original). You can tune these values and thus tune fidelity/diversity. Recall that having a lot of fidelity is not always the goalâ€”one failure mode of that is that you get one really real image but nothing else (no diversity), and that's not very interesting or successful from a model that's supposed to model the realm of all possible human faces or that of all possible coconutsâ€”including that of a cat pouncing after a flying coconut (but with extremely low probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe00ed",
   "metadata": {},
   "source": [
    "## Conditional GAN\n",
    "\n",
    "Labelled class data \n",
    "\n",
    "Generator:\n",
    "\n",
    "<img src=\"./images/cGAN_gen.png\" width=\"600\"/>\n",
    "\n",
    "Discriminator:\n",
    "\n",
    "<img src=\"./images/cGAN.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Controllable GAN\n",
    "\n",
    "Generate features by manipulating noise vector (i.e. lantent)\n",
    "\n",
    "<img src=\"./images/z_interpolation.png\" width=\"600\"/>\n",
    "\n",
    "$v_1 + d = v_2$\n",
    "\n",
    "- $v_1$ shows black hair\n",
    "- $v_2$ shows blue hair\n",
    "- vector $d$ shows direction to modify hair color\n",
    "\n",
    "Challenges:\n",
    "* Feature correlation: beard vs man\n",
    "* Z-space entanglement: element in z is related to multiple output features (e.g. not enough dimensions)\n",
    "\n",
    "Classifier gradients:\n",
    "<img src=\"./images/clas_grad.png\" width=\"600\"/>\n",
    "\n",
    "Classifiers can be used to find directions in z-space\n",
    "\n",
    "Disentanglement in z-space: supervised methods/ unsupervised methods\n",
    "\n",
    "E.g. Add regularization in loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621eb94",
   "metadata": {},
   "source": [
    "## GAN Evaluation\n",
    "\n",
    "GAN â€” How to measure GAN performance? (Hui, 2018): https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732\n",
    "\n",
    "Fidelity (quality of images) & Diversity (variety of images)\n",
    "\n",
    "There is no universal gold-standard discriminator.\n",
    "\n",
    "**Pixel Distance** is not reliable (e.g. shifting one pixel -> large pixel distance)\n",
    "\n",
    "**Feature Distance** (e.g. 2 eyes, 2 ears, 5 legs, ...)\n",
    "\n",
    "Extentively pre-trained classifiers available to use (feature layer)<br>\n",
    "The last pooling layer is the most commomly used for feature extraction\n",
    "\n",
    "ImageNet (>14 million images, > 20,000 categories)\n",
    "\n",
    "### Inception-v3\n",
    "\n",
    "Architecture: input (299x299x3) -> 8x8x2048 -> pooling (8x8) -> 2048 values\n",
    "\n",
    "Embedding of x: image -> Inception-v3 -> extracted features \n",
    "\n",
    "Comparing Embeddings (i.e. features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd4f51",
   "metadata": {},
   "source": [
    "### Frechet Inception Distance (FID)\n",
    "\n",
    "https://nealjean.com/ml/frechet-inception-distance/\n",
    "\n",
    "#### Frechet distance\n",
    "\n",
    "Dog walker analogy: the least amount of leash you can give your dog without ever having to give them more slack during the walk.\n",
    "\n",
    "Can be applied to two curves or two distributions.\n",
    "\n",
    "Multivariate Normal Frechet Distance:<br>\n",
    "$||\\mu X - \\mu Y||^2 + Tr(\\sum X + \\sum Y - 2\\sqrt{\\sum X + \\sum Y})$<br>\n",
    "Tr: trace of a matrix, sum of its diagonal elements<br>\n",
    "function of means and covariance matrix\n",
    "\n",
    "Real and fake embeddings are two **multivariate** normal distributions\n",
    "\n",
    "Lower FID = Closer distributions\n",
    "\n",
    "#### FID shortcomings\n",
    "\n",
    "* Use pre-trained inception model, which may not capture all features\n",
    "* FID score is biased, needs a large sample size (larger sample size, better GAN performance seems to be)\n",
    "* Slow to run\n",
    "* Limited statistics used: only mean and covariance - assumed multivariate normal\n",
    "\n",
    "### Inception Score (IS)\n",
    "\n",
    "Used before FID, FID is getting more popular, IS is being replaced by FID\n",
    "\n",
    "IS uses inception model classification\n",
    "\n",
    "Fidelity (Low entropy): probability not scatters on other classes but only on a few classes\n",
    "\n",
    "Diversity (High entropy): not concentrated on certain classes\n",
    "\n",
    "**KL divergence**:  $D_{KL}(p(y|x)||p(y))=p(y|x) log (\\frac{p(y|x)}{p(y)})$\n",
    "\n",
    "from \n",
    "\n",
    "* Marginal distribution P(y): Diversity\n",
    "\n",
    "to\n",
    "\n",
    "* Conditional distribution P(y|x): Fidelity\n",
    "\n",
    "Higher score -> Low entropy -> better\n",
    "\n",
    "#### IS shortcomings\n",
    "\n",
    "* Can be exploited or gamed (e.g. if GAN only produces one image for one class)\n",
    "* IS only looks at generated samples\n",
    "* Can miss useful features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8fcec",
   "metadata": {},
   "source": [
    "### Sampling and Truncation\n",
    "\n",
    "GAN evaluation is sample-dependent\n",
    "\n",
    "During training, noise vector generated from normal distribution\n",
    "\n",
    "Truncation chops off the tail ends during testing, if you want higher fidelity, truncate near zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ca59a",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "<img src=\"./images/precision.png\" width=\"700\"/>\n",
    "\n",
    "The state of the art models can often be bad at precision as opposed to recall. It leads to that prediction space is a superset of real space. This is why the truncation trick can come in handy for downstream applications to eliminate the space outside real distribution area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44313367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
