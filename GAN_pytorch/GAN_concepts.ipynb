{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d130ab54",
   "metadata": {},
   "source": [
    "# Build Basic Generative Adversarial Networks (GANs)\n",
    "Deeplearning.ai study notes\n",
    "\n",
    "https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b0863",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "Differentiable non-linear function\n",
    "- differentiable for backpropagation\n",
    "- non-linear to compute complex features\n",
    "\n",
    "**ReLU**: $max(z,0)$, derivative at zero is set to zero, derivative in negative space is zero -> dying ReLU problem\n",
    "\n",
    "**Leaky ReLU**: slope a = 0.1, catching up popularity\n",
    "\n",
    "**Sigmoid**: output [0,1], sigmoid not often used in hidden layers because the tails of the sigmoid curve have zero derivative -> vanishing gradient problem\n",
    "\n",
    "**Tanh**: output [-1,1], keeps the sign of input z, same saturation problem as sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6bdcb9",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "Training data uses batch stats\n",
    "\n",
    "Test data uses training stats\n",
    "\n",
    "**Covariate shift**: changes in the distribution of one variable affect the distribution of relayed variables\n",
    "\n",
    "Batch normalization:\n",
    "1. reduce internal covariate shift\n",
    "2. smooth out cost function\n",
    "3. make neural network easier to train\n",
    "\n",
    "### Training\n",
    "\n",
    "In a batch, z -> normalized z by mean and std -> learnable scale factor $\\gamma$ + shift factor $\\beta$ -> y -> into activation function\n",
    "\n",
    "$\\gamma$ and $\\beta$ are learnable parameters to get the optimal dist.\n",
    "\n",
    "Not necessary zero mean and standard deviation of one\n",
    "\n",
    "### Testing\n",
    "\n",
    "Use running mean and standard deviation that was computed over the entire training set, fixed after training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b05646",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "Each filter tels different features of the image (eye filter, noise filter, ear filter...)\n",
    "\n",
    "Stride\n",
    "\n",
    "Padding: frame on the image, capture useful information at edge\n",
    "\n",
    "Pooling: lower the dimension of input images\n",
    "\n",
    "Upsampling: (use predifined methods)\n",
    "- Nearest Neighbors\n",
    "- Linear interpolation\n",
    "- Bi-linear interpolation\n",
    "\n",
    "**Transposed Convolutions**\n",
    "\n",
    "input * filter -> up-sampling with learned parameters\n",
    "\n",
    "issue: centered values are influenced much more heavily -> output has checkerboard problem\n",
    "\n",
    "solution: use up-sampling followed by conv -> popular technique t avoid checkerboard problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd728a",
   "metadata": {},
   "source": [
    "## Generative Models\n",
    "\n",
    "Model: Noise + Class -> predict -> Features\n",
    "\n",
    "* Variational Autoencoders (VAE)\n",
    "\n",
    "Training: img -> Encoder -> Latent Space -> Decoder -> img' (reconstruct image)\n",
    "\n",
    "Generating: random latent space -> Decoder -> generated img\n",
    "\n",
    "<img src=\"./images/vae_architecture.png\" width=\"600\"/>\n",
    "\n",
    "Decoder: ConvTranspose2d interpolation\n",
    "\n",
    "<img src=https://img-blog.csdnimg.cn/20190111154919282.gif width=\"200\"/>\n",
    "\n",
    "\n",
    "Generated images are generally blurrier than GAN, stable training\n",
    "\n",
    "* Generative Adversarial Networks (GAN)\n",
    "\n",
    "Training:\n",
    "\n",
    "random noise -> **Generator** -> generated img\n",
    "\n",
    "generated img -> **Discriminator** -> real/fake img\n",
    "\n",
    "Generating:\n",
    "\n",
    "random noise -> Generator -> generated img\n",
    "\n",
    "* Flow Models\n",
    "\n",
    "Use invertible mappings between noise and generative image\n",
    "\n",
    "* Hybrid Models\n",
    "\n",
    "\n",
    "* Score-based Generative Model\n",
    "\n",
    "Score-based generative models, a family of approaches based on estimating gradients of the data distribution, have obtained high-quality samples comparable to GANs (like below, figure from this paper) without requiring adversarial training, and are considered by some to be the new contender to GANs.\n",
    "\n",
    "### VQ-VAE\n",
    "\n",
    "It relies on autoregressive model: use previous pixels to determine next pixel (supervised learning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca5550",
   "metadata": {},
   "source": [
    "## Procedures\n",
    "\n",
    "Discriminator Training:\n",
    "\n",
    "<img src=\"./images/gan_train.png\" width=\"600\"/>\n",
    "\n",
    "Generator Training: (G wants to fool D as much as possible)\n",
    "\n",
    "<img src=\"./images/gan_train2.png\" width=\"600\"/>\n",
    "\n",
    "Both models should improve together and always be at a similar \"skill\" level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef05f4f",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy (BCE) Loss\n",
    "\n",
    "Deisgned for classification real/fake\n",
    "\n",
    "<img src=\"./images/BCE.png\" width=\"600\"/>\n",
    "\n",
    "- m: batch size\n",
    "- h: prediction made by model\n",
    "- y: true label\n",
    "- x: features (img)\n",
    "- $\\theta$: discriminator parameters\n",
    "\n",
    "when prediction is 0 -> left part = 0<br>\n",
    "when prediction is 1 -> left part tells if h = 0, loss = inf; if h = 1, loss = 0\n",
    "\n",
    "when prediction is 1 -> right part = 0<br>\n",
    "when prediction is 0 -> right part tells if h = 0, loss = 0; if h = 1, loss = inf\n",
    "\n",
    "J -> 0 when label and prediction are close<br>\n",
    "J -> inf when label and prediction are different\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e704dcf",
   "metadata": {},
   "source": [
    "### Problem with BCE\n",
    "\n",
    "When discriminator improves too much, the function approximated by BCE loss will contain flat regions.\n",
    "\n",
    "Flat region on the cost function = vanishing gradients\n",
    "\n",
    "The discriminator does not output useful gradients (feedback) for the generator when the real/fake distributions are far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c8aa7",
   "metadata": {},
   "source": [
    "### Model Collapse\n",
    "\n",
    "Mode: any peak on the density function is a model\n",
    "\n",
    "<img src=\"./images/multi-mode.png\" width=\"600\"/>\n",
    "\n",
    "Real-world datasets have many modes related to each possible class\n",
    "\n",
    "Mode collapse happens when generataor gets stuck in one mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af40ff1",
   "metadata": {},
   "source": [
    "## Earth Mover's Distance (EMD)\n",
    "\n",
    "It measures the distance between two distributions.\n",
    "\n",
    "Amount of efforts to make the generated distribution equal to the real distribution.\n",
    "\n",
    "Example: amount of dirt moved * moving distance to move and transform a pile of dirct in the shape of distribution P to the shape of distribution Q. \n",
    "\n",
    "A recursion problem: $cost_{i+1} = cost_{i} + P_i - Q_i$.\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/EM_distance_discrete.png\n",
    "\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n",
    "No saturation when two distributions are very far apart\n",
    "\n",
    "## Wasserstein GAN\n",
    "\n",
    "W-Loss = $\\mathbb{E}(c(x)-\\mathbb{E}(c(g(z))))$<br>\n",
    "W-Loss helps with mode collapse and vanishing gradient problems\n",
    "\n",
    "### Condition on W-Loss\n",
    "\n",
    "Critic should be 1-Lipschitz continuous, norm of its gradient needs t be at most one.<br>\n",
    "Needed for training stable neural networks with W-Loss.\n",
    "\n",
    "### Weight Clipping\n",
    "Force the weights of critic to a fixed interval<br>\n",
    "Limits the learning ability of critic\n",
    "\n",
    "### Gradient Penalty\n",
    "Softer way, add regularization term - a two sided-penalty on gradients. In practice, a random interpolation image generated from real and fake images are used to calculate the gradient. \n",
    "\n",
    "<img src=\"./images/WGAN.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The authors recommended **RMSProp optimizer** on the critic, rather than a momentum based optimizer such as Adam which could cause instability in the model training. No theoretical explanation yet.\n",
    "\n",
    "### Spectral Normalization\n",
    "\n",
    "Spectral Normalization for Generative Adversarial Networks (Miyato et al. 2018)<br>\n",
    "https://arxiv.org/abs/1802.05957\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb2dc7",
   "metadata": {},
   "source": [
    "## Noise vector $z$\n",
    "\n",
    "The noise vector $z$ has the important role of making sure the images generated from the same class $y$ don't all look the same—think of it as a random seed. You generate it randomly, usually by sampling random numbers either between 0 and 1 uniformly, or from the normal distribution, which you can denote $z$ ~ $N(0, 1)$. The zero means the normal distribution has a mean of zero, and the 1 means that the normal distribution has a variance of 1. \n",
    "\n",
    "In reality, $z$ is usually larger than just 1 value to allow for more combinations of what $z$ could be. There's no special number that determines what works, but 100 is standard. Some researchers might use a power of 2, like 128 or 512, but again, nothing special about the number itself, just that it's large enough to contain a lot of possibilities. As a result, you would sample $z$ from that many different dimensions (constituting multiple normal distributions).\n",
    "\n",
    "*Fun Fact: this is also called a spherical normal and denoted $z$ ~ $N(0, I)$ where the $I$ represents the identity matrix and means the variance is 1 in all dimensions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b68e54",
   "metadata": {},
   "source": [
    "## Truncation trick\n",
    "\n",
    "So now that you're a bit familiar with noise vectors, here's another cool concept that people use to tune their outputs. It's called the truncation trick. I like to think of the truncation trick as a way of trading off fidelity (quality) and diversity in the samples. It works like this: when you randomly sample your noise vector $z$, you can choose to keep that random $z$ or you can sample another one. \n",
    "\n",
    "Why would you want to sample another one? \n",
    "\n",
    "Well, since I'm sampling $z$ from a normal distribution, my model will see more of those $z$ values within a standard deviation from the mean than those at the tails of the distribution—and this happens during training. This means that while the model is training, it's likely to be familiar with certain noise vectors and as a result model those areas coming from familiar noise vector regions. In these areas, my model will likely have much more realistic results, but nothing too funky, it's not taking as many risks in those regions mapped from those familiar noise vectors. This is the trade-off between fidelity (realistic, high quality images) and diversity (variety in images). \n",
    "\n",
    "What the truncation trick does is resamples the noise vector $z$ until it falls within some bounds of the normal distribution. In fact, it samples $z$ from a truncated normal distribution where the tails are cut off at different values (red line in graph is truncated normal, blue is original). You can tune these values and thus tune fidelity/diversity. Recall that having a lot of fidelity is not always the goal—one failure mode of that is that you get one really real image but nothing else (no diversity), and that's not very interesting or successful from a model that's supposed to model the realm of all possible human faces or that of all possible coconuts—including that of a cat pouncing after a flying coconut (but with extremely low probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe00ed",
   "metadata": {},
   "source": [
    "## Conditional GAN\n",
    "\n",
    "Labelled class data \n",
    "\n",
    "Generator:\n",
    "\n",
    "<img src=\"./images/cGAN_gen.png\" width=\"600\"/>\n",
    "\n",
    "Discriminator:\n",
    "\n",
    "<img src=\"./images/cGAN.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Controllable GAN\n",
    "\n",
    "Generate features by manipulating noise vector (i.e. lantent)\n",
    "\n",
    "<img src=\"./images/z_interpolation.png\" width=\"600\"/>\n",
    "\n",
    "$v_1 + d = v_2$\n",
    "\n",
    "- $v_1$ shows black hair\n",
    "- $v_2$ shows blue hair\n",
    "- vector $d$ shows direction to modify hair color\n",
    "\n",
    "Challenges:\n",
    "* Feature correlation: beard vs man\n",
    "* Z-space entanglement: element in z is related to multiple output features (e.g. not enough dimensions)\n",
    "\n",
    "Classifier gradients:\n",
    "<img src=\"./images/clas_grad.png\" width=\"600\"/>\n",
    "\n",
    "Classifiers can be used to find directions in z-space\n",
    "\n",
    "Disentanglement in z-space: supervised methods/ unsupervised methods\n",
    "\n",
    "E.g. Add regularization in loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e1696",
   "metadata": {},
   "source": [
    "## GAN Evaluation\n",
    "\n",
    "GAN — How to measure GAN performance? (Hui, 2018): https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732\n",
    "\n",
    "Fidelity (quality of images) & Diversity (variety of images)\n",
    "\n",
    "There is no universal gold-standard discriminator.\n",
    "\n",
    "**Pixel Distance** is not reliable (e.g. shifting one pixel -> large pixel distance)\n",
    "\n",
    "**Feature Distance** (e.g. 2 eyes, 2 ears, 5 legs, ...)\n",
    "\n",
    "Extentively pre-trained classifiers available to use (feature layer)<br>\n",
    "The last pooling layer is the most commomly used for feature extraction\n",
    "\n",
    "ImageNet (>14 million images, > 20,000 categories)\n",
    "\n",
    "### Inception-v3\n",
    "\n",
    "Architecture: input (299x299x3) -> 8x8x2048 -> pooling (8x8) -> 2048 values\n",
    "\n",
    "Embedding of x: image -> Inception-v3 -> extracted features \n",
    "\n",
    "Comparing Embeddings (i.e. features)\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/inceptionv3onc--oview_vjAbOfw.png\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7959c",
   "metadata": {},
   "source": [
    "### Frechet Inception Distance (FID)\n",
    "\n",
    "https://nealjean.com/ml/frechet-inception-distance/\n",
    "\n",
    "#### Frechet distance\n",
    "\n",
    "Dog walker analogy: the least amount of leash you can give your dog without ever having to give them more slack during the walk.\n",
    "\n",
    "Can be applied to two curves or two distributions.\n",
    "\n",
    "Multivariate Normal Frechet Distance:<br>\n",
    "$||\\mu X - \\mu Y||^2 + Tr(\\sum X + \\sum Y - 2\\sqrt{\\sum X + \\sum Y})$<br>\n",
    "Tr: trace of a matrix, sum of its diagonal elements<br>\n",
    "function of means and covariance matrix\n",
    "\n",
    "Real and fake embeddings are two **multivariate** normal distributions\n",
    "\n",
    "Lower FID = Closer distributions\n",
    "\n",
    "#### FID shortcomings\n",
    "\n",
    "* Use pre-trained inception model, which may not capture all features\n",
    "* FID score is biased, needs a large sample size (larger sample size, better GAN performance seems to be)\n",
    "* Slow to run\n",
    "* Limited statistics used: only mean and covariance - assumed multivariate normal\n",
    "\n",
    "### Inception Score (IS)\n",
    "\n",
    "Used before FID, FID is getting more popular, IS is being replaced by FID\n",
    "\n",
    "IS uses inception model classification\n",
    "\n",
    "Fidelity (Low entropy): probability not scatters on other classes but only on a few classes\n",
    "\n",
    "Diversity (High entropy): not concentrated on certain classes\n",
    "\n",
    "**KL divergence**:  $D_{KL}(p(y|x)||p(y))=p(y|x) log (\\frac{p(y|x)}{p(y)})$\n",
    "\n",
    "from \n",
    "\n",
    "* Marginal distribution P(y): Diversity\n",
    "\n",
    "to\n",
    "\n",
    "* Conditional distribution P(y|x): Fidelity\n",
    "\n",
    "Higher score -> Low entropy -> better\n",
    "\n",
    "#### IS shortcomings\n",
    "\n",
    "* Can be exploited or gamed (e.g. if GAN only produces one image for one class)\n",
    "* IS only looks at generated samples\n",
    "* Can miss useful features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc248534",
   "metadata": {},
   "source": [
    "### Sampling and Truncation\n",
    "\n",
    "GAN evaluation is sample-dependent\n",
    "\n",
    "During training, noise vector generated from normal distribution\n",
    "\n",
    "Truncation chops off the tail ends during testing, if you want higher fidelity, truncate near zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ed2b9",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "<img src=\"./images/precision.png\" width=\"700\"/>\n",
    "\n",
    "The state of the art models can often be bad at precision as opposed to recall. It leads to that prediction space is a superset of real space. This is why the truncation trick can come in handy for downstream applications to eliminate the space outside real distribution area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46247ffd",
   "metadata": {},
   "source": [
    "## Bias in the model\n",
    "\n",
    "* Training data bias (collection methods, location, diversity of labellers...)\n",
    "* Evaluation bias (dominant culture, social standards...)\n",
    "* Model architecture bias (loss function...)\n",
    "* can come from every step\n",
    "\n",
    "\n",
    "## PULSE project\n",
    "\n",
    "Upsampling: add pixels, pixelated image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e55de",
   "metadata": {},
   "source": [
    "## Introduction to Score-Based Models\n",
    "\n",
    "### Score and Score-Based Models\n",
    "Given a probablity density function $p(\\mathbf{x})$, we define the *score* as $$\\nabla_\\mathbf{x} \\log p(\\mathbf{x}).$$ As you might guess, score-based generative models are trained to estimate $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$. Unlike likelihood-based models such as flow models or autoregressive models, score-based models do not have to be normalized and are easier to parameterize. For example, consider a non-normalized statistical model $p_\\theta(\\mathbf{x}) = \\frac{e^{-E_\\theta(\\mathbf{x})}}{Z_\\theta}$, where $E_\\theta(\\mathbf{x}) \\in \\mathbb{R}$ is called the energy function and $Z_\\theta$ is an unknown normalizing constant that makes $p_\\theta(\\mathbf{x})$ a proper probability density function. The energy function is typically parameterized by a flexible neural network. When training it as a likelihood model, we need to know the normalizing constant $Z_\\theta$ by computing complex high-dimensional integrals, which is typically intractable. In constrast, when computing its score, we obtain $\\nabla_\\mathbf{x} \\log p_\\theta(\\mathbf{x}) = -\\nabla_\\mathbf{x} E_\\theta(\\mathbf{x})$ which does not require computing the normalizing constant $Z_\\theta$.\n",
    "\n",
    "In fact, any neural network that maps an input vector $\\mathbf{x} \\in \\mathbb{R}^d$ to an output vector $\\mathbf{y} \\in \\mathbb{R}^d$ can be used as a score-based model, as long as the output and input have the same dimensionality. This yields huge flexibility in choosing model architectures.\n",
    "\n",
    "### Perturbing Data with a Diffusion Process\n",
    "\n",
    "In order to generate samples with score-based models, we need to consider a [diffusion process](https://en.wikipedia.org/wiki/Diffusion_process) that corrupts data slowly into random noise. Scores will arise when we reverse this diffusion process for sample generation. You will see this later in the notebook.\n",
    "\n",
    "A diffusion process is a [stochastic process](https://en.wikipedia.org/wiki/Stochastic_process#:~:text=A%20stochastic%20or%20random%20process%20can%20be%20defined%20as%20a,an%20element%20in%20the%20set.) similar to [Brownian motion](https://en.wikipedia.org/wiki/Brownian_motion). Their paths are like the trajectory of a particle submerged in a flowing fluid, which moves randomly due to unpredictable collisions with other particles. Let $\\{\\mathbf{x}(t) \\in \\mathbb{R}^d \\}_{t=0}^T$ be a diffusion process, indexed by the continuous time variable $t\\in [0,T]$. A diffusion process is governed by a stochastic differential equation (SDE), in the following form\n",
    "\n",
    "\\begin{align*}\n",
    "d \\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) d t + g(t) d \\mathbf{w},\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{f}(\\cdot, t): \\mathbb{R}^d \\to \\mathbb{R}^d$ is called the *drift coefficient* of the SDE, $g(t) \\in \\mathbb{R}$ is called the *diffusion coefficient*, and $\\mathbf{w}$ represents the standard Brownian motion. You can understand an SDE as a stochastic generalization to ordinary differential equations (ODEs). Particles moving according to an SDE not only follows the deterministic drift $\\mathbf{f}(\\mathbf{x}, t)$, but are also affected by the random noise coming from $g(t) d\\mathbf{w}$. \n",
    "\n",
    "For score-based generative modeling, we will choose a diffusion process such that $\\mathbf{x}(0) \\sim p_0$, where we have a dataset of i.i.d. samples, and $\\mathbf{x}(T) \\sim p_T$, for which we have a tractable form to sample from.\n",
    "\n",
    "### Reversing the Diffusion Process Yields Score-Based Generative Models\n",
    "By starting from a sample from $p_T$ and reversing the diffusion process, we will be able to obtain a sample from $p_\\text{data}$. Crucially, the reverse process is a diffusion process running backwards in time. It is given by the following reverse-time SDE\n",
    "\n",
    "\\begin{align}\n",
    "  d\\mathbf{x} = [\\mathbf{f}(\\mathbf{x}, t) - g^2(t)\\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x})] dt + g(t) d\\bar{\\mathbf{w}},\n",
    "\\end{align}\n",
    "\n",
    "where $\\bar{\\mathbf{w}}$ is a Brownian motion in the reverse time direction, and $dt$ here represents an infinitesimal negative time step. Here $p_t(\\mathbf{x})$ represents the distribution of $\\mathbf{x}(t)$. This reverse SDE can be computed once we know the drift and diffusion coefficients of the forward SDE, as well as the score of $p_t(\\mathbf{x})$ for each $t\\in[0, T]$.\n",
    "\n",
    "The overall intuition of score-based generative modeling with SDEs can be summarized in the illustration below\n",
    "![sde schematic](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/diffusion_schematic.jpg?raw=true)\n",
    "\n",
    "### Score Estimation\n",
    "\n",
    "Based on the above intuition, we can use the time-dependent score function $\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})$ to construct the reverse-time SDE, and then solve it numerically to obtain samples from $p_0$ using samples from a prior distribution $p_T$. We can train a time-dependent score-based model $s_\\theta(\\mathbf{x}, t)$ to approximate $\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})$, using the following weighted sum of [denoising score matching](http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf) objectives.\n",
    "\n",
    "\\begin{align}\n",
    "\\min_\\theta \\mathbb{E}_{t\\sim \\mathcal{U}(0, T)} [\\lambda(t) \\mathbb{E}_{\\mathbf{x}(0) \\sim p_0(\\mathbf{x})}\\mathbf{E}_{\\mathbf{x}(t) \\sim p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))}[ \\|s_\\theta(\\mathbf{x}(t), t) - \\nabla_{\\mathbf{x}(t)}\\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))\\|_2^2]],\n",
    "\\end{align}\n",
    "where $\\mathcal{U}(0,T)$ is a uniform distribution over $[0, T]$, $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$ denotes the transition probability from $\\mathbf{x}(0)$ to $\\mathbf{x}(t)$, and $\\lambda(t) \\in \\mathbb{R}^+$ denotes a continuous weighting function.\n",
    "\n",
    "In the objective, the expectation over $\\mathbf{x}(0)$ can be estimated with empirical means over data samples from $p_0$. The expectation over $\\mathbf{x}(t)$ can be estimated by sampling from $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$, which is efficient when the drift coefficient $\\mathbf{f}(\\mathbf{x}, t)$ is affine. The weight function $\\lambda(t)$ is typically chosen to be inverse proportional to $\\mathbb{E}[\\|\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0)) \\|_2^2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b9d57",
   "metadata": {},
   "source": [
    "## Train on GAN Data\n",
    "\n",
    "<img src=\"./images/train_on_GAN.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a2af0",
   "metadata": {},
   "source": [
    "## GAN Applications\n",
    "\n",
    "* GauGAN: drawing style translation\n",
    "* Super-Resolution GAN: blurr image to sharpened image\n",
    "* Multimodal image-to-image translation: cat to different dogs\n",
    "* Text-to-Image\n",
    "* Image-and-landmark-to-video: face landmarks\n",
    "* Image filters: make-up, animal face\n",
    "* Image editing: change image mask to edit image\n",
    "* Art styles transfer\n",
    "* Data augmentation\n",
    "* Climate change: visualize climate warming\n",
    "* Media: deepface\n",
    "* Adversarial examples & robustness: fool other AI application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407200e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
